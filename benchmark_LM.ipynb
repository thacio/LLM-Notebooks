{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTzNimfiFamU"
      },
      "source": [
        "# Inicialização das variáveis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tphHGol9EjPU"
      },
      "outputs": [],
      "source": [
        "RESUME_FROM_CHECKPOINT=None\n",
        "\n",
        "inserir_beginoftext_token = True # Inserir um token '<|target_bos|>' separando o prompt da resposta nos modelos tipo GPT\n",
        "\n",
        "# Pode ser necessário um valor maior para tarefas de sumarização\n",
        "MAX_TOKEN_GENERATION_LENGTH=60\n",
        "\n",
        "output_dir=\"./fine-tuned-model\"\n",
        "## É necessário especificar o tipo de arquitetura\n",
        "# model_type='decoder'\n",
        "# model_type='encoder-decoder'\n",
        "\n",
        "epochs = 2\n",
        "dropout_rate=0.1\n",
        "BATCH_SIZE = 64\n",
        "EVAL_BATCH_SIZE=64\n",
        "\n",
        "if not RESUME_FROM_CHECKPOINT:\n",
        "    transformer_model_name='tgsc/ult5-pt-small'; model_type='encoder-decoder'; dropout_rate=0.0; BATCH_SIZE = 64; EVAL_BATCH_SIZE=64; #prefix_input='<|NLU|>' # <|NLG|>\n",
        "\n",
        "    # transformer_model_name='pierreguillou/gpt2-small-portuguese'; model_type='decoder'; BATCH_SIZE = 16; EVAL_BATCH_SIZE=16\n",
        "\n",
        "    # transformer_model_name='unicamp-dl/ptt5-small-portuguese-vocab'; model_type='encoder-decoder'; BATCH_SIZE = 64; EVAL_BATCH_SIZE=64;\n",
        "    # transformer_model_name='unicamp-dl/ptt5-base-portuguese-vocab'; model_type='encoder-decoder'; BATCH_SIZE = 10; EVAL_BATCH_SIZE=10\n",
        "    # transformer_model_name='unicamp-dl/ptt5-large-portuguese-vocab'; model_type='encoder-decoder'; BATCH_SIZE = 5; EVAL_BATCH_SIZE=5    \n",
        "\n",
        "gradient_accumulation_steps = int(round(128//BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znCm5mx5DIBI"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "num_proc = multiprocessing.cpu_count()\n",
        "print('cpu_count:',num_proc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGBw38YAFgyK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate\n",
        "!pip install sentencepiece\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZogJiFyFlta"
      },
      "source": [
        "# Carrega o tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO9Xs47eFYvA"
      },
      "outputs": [],
      "source": [
        "import transformers as transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, T5Tokenizer\n",
        "\n",
        "# Carrega o tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
        "\n",
        "# Nos modelos decoder, adicionaremos um token separando a entrada da resposta, para o modelo identificar que é para fazer a tarefa\n",
        "if model_type=='decoder':\n",
        "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
        "    # tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "\n",
        "    if inserir_beginoftext_token:\n",
        "        target_bos_token='<|target_bos|>'\n",
        "        tokenizer.add_special_tokens({ \"additional_special_tokens\": [target_bos_token] })\n",
        "\n",
        "# # Descomentar para adicionar tokens no vocabulário do tokenizer para virar múltiplo de 8 (recomendação NVIDIA para uso dos Tensor Cores com fp16)\n",
        "# i = 0\n",
        "# while len(tokenizer) % 8 !=0 :\n",
        "#     tokenizer.add_special_tokens({ \"additional_special_tokens\": [f\"<|vocab_pad_{i}\"] })\n",
        "#     i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km6v-iUFH3ha"
      },
      "source": [
        "# Carrega o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5geNQlnH248"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModel, AutoModelForSeq2SeqLM, AutoModelForCausalLM, BertLMHeadModel\n",
        "import torch\n",
        "\n",
        "if model_type=='encoder-decoder':\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(transformer_model_name,dropout_rate=dropout_rate)\n",
        "elif model_type=='decoder':\n",
        "    model = AutoModelForCausalLM.from_pretrained(transformer_model_name)\n",
        "else:\n",
        "    raise ValueError('tipo de arquitetura deve ser \"decoder\" ou \"encoder-decoder')\n",
        "\n",
        "# Resize no modelo para o novo vocabulário\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.max_length=MAX_TOKEN_GENERATION_LENGTH\n",
        "\n",
        "print(model.config)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "try:\n",
        "    context_length=model.config.n_positions\n",
        "except:\n",
        "    context_length=model.config.max_position_embeddings\n",
        "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcx8IzeBaQM4"
      },
      "source": [
        "# Carrega os datasets e cria as métricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru6PzSQS_B0v"
      },
      "source": [
        "Para adicionar o dataset ao benchmark, basta descomentar o bloco (atalho CTRL + \";\" )\n",
        "\n",
        "Datasets\n",
        "\n",
        "```\n",
        "Perguntas e respostas: SQUAD\n",
        "Correção semântica/gramatical: COLA\n",
        "Implicações lógicas: ASSIN 2, RTE, SCITAIL, MLNI, SNLI\n",
        "Paráfrase: MRPC e QQP\n",
        "Score de similaridade: ASSIN 2, STSB\n",
        "Classificação de senimentos: SST2\n",
        "Sumarização: WiKI LINGUA e XLSUM\n",
        "```\n",
        "\n",
        "Grupos recomendados para fine-tune em conjunto (devido ao grande tamanho de alguns datasets)\n",
        "```\n",
        "[ASSIN 2, COLA, MRPC, RTE, STSB], SCITAIL opcionalmente\n",
        "[SQUAD V1.1]\n",
        "[WIKI LINGUA, XLSUM]\n",
        "[SNLI,MLNI]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-G81AKMa5kg"
      },
      "outputs": [],
      "source": [
        "ds = {}\n",
        "ds_processado = {}\n",
        "ds_tokenizado = {}\n",
        "\n",
        "# cria um dicionário dataset para poder identificar qual o benchmark durante o cálculo das métricas\n",
        "identificador_de_dataset = {}\n",
        "metric_functions = {}\n",
        "map_functions = {}\n",
        "\n",
        "# o contador identificará qual o dataset no data_collator do modelo\n",
        "if model_type==\"encoder-decoder\":\n",
        "    contador= 10_000\n",
        "elif model_type=='decoder':\n",
        "    contador=len(tokenizer) + 10_000\n",
        "\n",
        "def criar_identificador():\n",
        "    global contador\n",
        "    contador += 1\n",
        "    return contador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XIFfFG1Rwgv"
      },
      "source": [
        "## ASSIN 2\n",
        "\n",
        "https://sites.google.com/view/assin2\n",
        "\n",
        "https://huggingface.co/datasets/assin2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz0ebJslOPCa"
      },
      "source": [
        "### Métrica calculada pelo código do script de avaliação\n",
        "https://github.com/erickrf/assin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPTEWP62L7hl"
      },
      "outputs": [],
      "source": [
        "# Código ajustado de https://github.com/erickrf/assin\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "class Pair(object):\n",
        "    '''\n",
        "    Class representing a pair of texts from SICK or RTE.\n",
        "    It is meant to be used as an abstract representation for both.\n",
        "    '''\n",
        "    def __init__(self, entailment, similarity):\n",
        "    # def __init__(self, t, h, id_, entailment, similarity):\n",
        "        '''\n",
        "        :param t: string with the text\n",
        "        :param h: string with the hypothesis\n",
        "        :param id_: int indicating id in the original file\n",
        "        :param entailment: int indicating entailment class\n",
        "        :param similarity: float\n",
        "        '''\n",
        "        # self.t = t\n",
        "        # self.h = h\n",
        "        # self.id = id_\n",
        "        self.entailment = entailment\n",
        "        self.similarity = similarity\n",
        "\n",
        "def eval_rte(pairs_gold, pairs_sys):\n",
        "    '''\n",
        "    Evaluate the RTE output of the system against a gold score. \n",
        "    Results are printed to stdout.\n",
        "    '''\n",
        "    # check if there is an entailment value\n",
        "    if pairs_sys[0].entailment is None:\n",
        "        print()\n",
        "        print('No RTE output to evaluate')\n",
        "        return\n",
        "    \n",
        "    gold_values = np.array([p.entailment for p in pairs_gold])\n",
        "    sys_values = np.array([p.entailment for p in pairs_sys])\n",
        "    label_set = set(gold_values)\n",
        "    macro_f1 = f1_score(gold_values, sys_values, average='macro', \n",
        "                        labels=list(label_set))\n",
        "    accuracy = (gold_values == sys_values).sum() / len(gold_values)\n",
        "    \n",
        "    return accuracy, macro_f1\n",
        "    # print()\n",
        "    # print('RTE evaluation')\n",
        "    # print('Accuracy\\tMacro F1')\n",
        "    # print('--------\\t--------')\n",
        "    # print('{:8.2%}\\t{:8.3f}'.format(accuracy, macro_f1))\n",
        "\n",
        "def eval_similarity(pairs_gold, pairs_sys):\n",
        "    '''\n",
        "    Evaluate the semantic similarity output of the system against a gold score. \n",
        "    Results are printed to stdout.\n",
        "    '''\n",
        "    # check if there is an entailment value\n",
        "    if pairs_sys[0].similarity is None:\n",
        "        print()\n",
        "        print('No similarity output to evaluate')\n",
        "        return\n",
        "    \n",
        "    gold_values = np.array([p.similarity for p in pairs_gold])\n",
        "    sys_values = np.array([p.similarity for p in pairs_sys])\n",
        "    pearson = pearsonr(gold_values, sys_values)[0]\n",
        "    absolute_diff = gold_values - sys_values\n",
        "    mse = (absolute_diff ** 2).mean()\n",
        "    \n",
        "    return pearson, mse\n",
        "    # print()\n",
        "    # print('Similarity evaluation')\n",
        "    # print('Pearson\\t\\tMean Squared Error')\n",
        "    # print('-------\\t\\t------------------')\n",
        "    # print('{:7.3f}\\t\\t{:18.2f}'.format(pearson, mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CVYSWxWSMgG"
      },
      "source": [
        "### Similarity score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi8RRCSLSOnH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from evaluate import load\n",
        "\n",
        "def assin2_score_metric(predictions,labels):\n",
        "    pairs_labels = []\n",
        "    pairs_preds = []\n",
        "\n",
        "    for i in range(0,len(predictions)):\n",
        "        # Busca o número na string gerada\n",
        "        pred_numbers = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\",predictions[i])\n",
        "        \n",
        "        # Caso não haja número, atribuir valor máximo e mínimo para penalizar o erro\n",
        "        if len(pred_numbers)==0:\n",
        "            predictions[i]=float(0)\n",
        "            labels[i]=float(5)\n",
        "        else:\n",
        "            try:\n",
        "                predictions[i] = float(pred_numbers[0])\n",
        "                labels[i] = float(labels[i])\n",
        "            except:\n",
        "                predictions[i]=float(0)\n",
        "                labels[i]=float(5)\n",
        "\n",
        "        if predictions[i]>5:\n",
        "            predictions[i]=5\n",
        "        elif predictions[i]<0:\n",
        "            predictions[i]=0\n",
        "\n",
        "        pairs_labels.append(Pair( 0, labels[i]))\n",
        "        pairs_preds.append(Pair( 0, predictions[i]))\n",
        "\n",
        "    pearson, mse = eval_similarity(pairs_labels, pairs_preds)\n",
        "\n",
        "    return {'assin2_score_pearson': pearson, 'assin2_score_mse': mse}\n",
        "\n",
        "def assin2_score_map_fc(examples):\n",
        "    new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "    first_key=list(examples.keys())[0]\n",
        "    for i in range(0,len(examples[first_key])):\n",
        "        input=f'assin2_similaridade premissa: {examples[\"premise\"][i]}'\n",
        "        input+=f' hipótese: {examples[\"hypothesis\"][i]}'\n",
        "\n",
        "        if examples['relatedness_score'][i] == -1:\n",
        "            continue\n",
        "        else:\n",
        "            label = ' ' + str(examples['relatedness_score'][i])\n",
        "            # label = 'pontuação de similaridade: ' + str(examples['label'][i])\n",
        "\n",
        "        if model_type=='decoder':\n",
        "            if inserir_beginoftext_token:\n",
        "                input += target_bos_token\n",
        "            else:\n",
        "                label = ' pontuação de similaridade: ' + label\n",
        "        label += tokenizer.eos_token\n",
        "\n",
        "        new_examples['text'].append(input)\n",
        "        new_examples['labels'].append(label)\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "import datasets\n",
        "ds['assin2_score'] = datasets.load_dataset(\"assin2\",cache_dir=\"./cache\")\n",
        "# No caso do assin2, substituir o split de validação pelo de teste\n",
        "ds['assin2_score']['validation']=ds['assin2_score']['test']\n",
        "del ds['assin2_score']['test']\n",
        "\n",
        "identificador_de_dataset['assin2_score']=criar_identificador()\n",
        "metric_functions['assin2_score'] = assin2_score_metric\n",
        "map_functions['assin2_score'] = assin2_score_map_fc\n",
        "\n",
        "new_features = ds['assin2_score']['train'].features.copy()\n",
        "new_features['relatedness_score'] = datasets.Value(dtype='string', id=None)\n",
        "ds['assin2_score'] = ds['assin2_score'].cast(new_features)\n",
        "\n",
        "ds['assin2_score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRNLDpLlSJTx"
      },
      "source": [
        "### Entailment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtpjmcshRv7P"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "def assin2_entail_metric(predictions,labels):\n",
        "\n",
        "    pairs_labels = []\n",
        "    pairs_preds = []\n",
        "\n",
        "    for i in range(0,len(labels)):\n",
        "        if labels[i]=='Não é implicação lógica':\n",
        "            labels[i]=0\n",
        "        elif labels[i]=='É implicação lógica':\n",
        "            labels[i]=1\n",
        "\n",
        "        if predictions[i]=='Não é implicação lógica':\n",
        "            predictions[i]=0\n",
        "        elif predictions[i]=='É implicação lógica':\n",
        "            predictions[i]=1\n",
        "        else: \n",
        "            # Como a predição é errada, colocamos o contrário do label\n",
        "            predictions[i]=0\n",
        "            if labels[i]==0:\n",
        "                predictions[i]=1\n",
        "\n",
        "        pairs_labels.append(Pair( labels[i], 0))\n",
        "        pairs_preds.append(Pair( predictions[i], 0))\n",
        "\n",
        "    accuracy, macro_f1 = eval_rte(pairs_labels, pairs_preds)\n",
        "\n",
        "    return {'assin2_entail_acc': accuracy, 'assin2_entail_f1' : macro_f1}\n",
        "\n",
        "\n",
        "def assin2_entail_map_fc(examples):\n",
        "    new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "    first_key=list(examples.keys())[0]\n",
        "    for i in range(0,len(examples[first_key])):\n",
        "        input=f'assin2_entail premissa: {examples[\"premise\"][i]}'\n",
        "        input+=f' hipótese: {examples[\"hypothesis\"][i]}'\n",
        "        if examples['entailment_judgment'][i] == 0:\n",
        "            label = 'Não é implicação lógica'\n",
        "        elif examples['entailment_judgment'][i] == 1:\n",
        "            label = 'É implicação lógica'\n",
        "\n",
        "        if model_type=='decoder':\n",
        "            if inserir_beginoftext_token:\n",
        "                input += target_bos_token\n",
        "            else:\n",
        "                label = ' julgamento: ' + label\n",
        "        label += tokenizer.eos_token\n",
        "\n",
        "        new_examples['text'].append(input)\n",
        "        new_examples['labels'].append(label)\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "import datasets\n",
        "ds['assin2_entail'] = datasets.load_dataset(\"assin2\",cache_dir=\"./cache\")\n",
        "\n",
        "# No caso do assin2, substituir o split de validação pelo de teste\n",
        "ds['assin2_entail']['validation']=ds['assin2_entail']['test']\n",
        "del ds['assin2_entail']['test']\n",
        "\n",
        "identificador_de_dataset['assin2_entail']=criar_identificador()\n",
        "metric_functions['assin2_entail'] = assin2_entail_metric\n",
        "map_functions['assin2_entail'] = assin2_entail_map_fc\n",
        "ds['assin2_entail']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzJps2dSZYyU"
      },
      "source": [
        "## SQUAD v1.1\n",
        "\n",
        "Traduzido pelo grupo Deep Learning Brasil (http://www.deeplearningbrasil.com.br/)\n",
        "\n",
        "Train: 87510 exemplos, Validation: 10570 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHzi5xiJZsb7"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# import re\n",
        "# import string\n",
        "# def normalize_answer(s):\n",
        "#     # https://github.com/huggingface/evaluate/blob/main/metrics/squad/compute_score.py\n",
        "#     \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "#     def remove_articles(text):\n",
        "#         return re.sub(r\"\\b(o|a|os|as|um|uns|uma|umas)\\b\", \" \", text)\n",
        "\n",
        "#     def white_space_fix(text):\n",
        "#         return \" \".join(text.split())\n",
        "\n",
        "#     def remove_punc(text):\n",
        "#         exclude = set(string.punctuation)\n",
        "#         return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "#     def lower(text):\n",
        "#         return text.lower()\n",
        "\n",
        "#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "# def squad_v11_metric(predictions,labels):\n",
        "\n",
        "#     for i in range(0,len(predictions)):\n",
        "#       predictions[i] = normalize_answer(predictions[i])\n",
        "\n",
        "#       predictions[i] = { 'prediction_text' : predictions[i] , 'id': labels[i]}\n",
        "#       labels[i] = { 'answers': { 'text' : ds_squad_val_dict[labels[i]]['text'], 'answer_start': ds_squad_val_dict[labels[i]]['answer_start']} , 'id': labels[i]}\n",
        "\n",
        "#       for j in range(0,len(labels[i]['answers']['text'])):\n",
        "#           labels[i]['answers']['text'][j] = normalize_answer(labels[i]['answers']['text'][j])\n",
        "\n",
        "#     squad_metric = load(\"squad\")\n",
        "#     result = squad_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "#     return {'squad_v1.1_acc': result['exact_match'], 'squad_v1.1_f1': result['f1']}\n",
        "\n",
        "# def squad_v11_map_fc(examples):    \n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "    \n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'squad Contexto: {examples[\"context\"][i]}'\n",
        "#         input+=f' Pergunta: {examples[\"question\"][i]}'\n",
        "#         # input+= ' A transcrição exata do trecho do contexto que responde a pergunta é:'\n",
        "\n",
        "#         label = examples['answers'][i]['text'][0]\n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' Resposta: ' + label\n",
        "\n",
        "#         label += tokenizer.eos_token\n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)        \n",
        "    \n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "\n",
        "# def squad_v11_map_validation_fc(examples):    \n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "    \n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'squad Contexto: {examples[\"context\"][i]}'\n",
        "#         input+=f' Pergunta: {examples[\"question\"][i]}'\n",
        "#         # input+= ' A transcrição exata do trecho do contexto que responde a pergunta é:'\n",
        "\n",
        "#         label = examples['id'][i]\n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)        \n",
        "    \n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "\n",
        "# ds['squad_v1.1'] = datasets.load_dataset(\"tgsc/squad-pt-v1.1\",cache_dir=\"./cache\")\n",
        "# ds_squad_val = ds['squad_v1.1']['validation']\n",
        "# ds_squad_val_dict = {} # dicionário para a avaliação do squad (os exemplos contêm lista de respostas)\n",
        "# for i in range(len(ds_squad_val)):\n",
        "#     ds_squad_val_dict[ds_squad_val[i]['id']]=ds_squad_val[i]['answers']\n",
        "\n",
        "# identificador_de_dataset['squad_v1.1']=criar_identificador()\n",
        "# metric_functions['squad_v1.1'] = squad_v11_metric\n",
        "# map_functions['squad_v1.1'] = {'train': squad_v11_map_fc, 'validation': squad_v11_map_validation_fc }\n",
        "\n",
        "# ds['squad_v1.1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1soMrkkkuwB"
      },
      "source": [
        "## PLUE - GLUE em português\n",
        "\n",
        "Versão em português\n",
        "\n",
        "https://huggingface.co/datasets/dlb/plue\n",
        "\n",
        "https://github.com/jubs12/PLUE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U118hTygaPFB"
      },
      "source": [
        "### COLA\n",
        "\n",
        "Train: 8551 exemplos, Validation: 1043 exemplos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzgmMJndaevu"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "def cola_metric(predictions,labels):\n",
        "    exact_match_metric = load(\"exact_match\")\n",
        "    result_acc = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "\n",
        "    glue_metric = load('glue', 'cola')\n",
        "\n",
        "    for i in range(0,len(labels)):\n",
        "        if labels[i]=='Não é gramaticalmente aceitável':\n",
        "            labels[i]=0\n",
        "        elif labels[i]=='Gramaticalmente correto':\n",
        "            labels[i]=1\n",
        "\n",
        "        if predictions[i]=='Não é gramaticalmente aceitável':\n",
        "            predictions[i]=0\n",
        "        elif predictions[i]=='Gramaticalmente correto':\n",
        "            predictions[i]=1\n",
        "        else: \n",
        "            # Como a predição é errada, colocamos o contrário do label\n",
        "            predictions[i]=0\n",
        "            if labels[i]==0:\n",
        "                predictions[i]=1\n",
        "\n",
        "    results = glue_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {'cola_acc': result_acc['exact_match'], 'cola_matthews_corr': results['matthews_correlation']}\n",
        "\n",
        "def cola_map_fc(examples):    \n",
        "    new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "    first_key=list(examples.keys())[0]\n",
        "    \n",
        "    for i in range(0,len(examples[first_key])):\n",
        "        input=f'cola sentença: {examples[\"sentence\"][i]}'\n",
        "        if examples['label'][i] == 0:\n",
        "            label = 'Não é gramaticalmente aceitável'\n",
        "        elif examples['label'][i] == 1:\n",
        "            label = 'Gramaticalmente correto'\n",
        "        elif examples['label'][i] == -1:\n",
        "            continue\n",
        "\n",
        "\n",
        "        if model_type=='decoder':\n",
        "            if inserir_beginoftext_token:\n",
        "                input += target_bos_token\n",
        "            else:\n",
        "                label = ' classe: ' + label\n",
        "\n",
        "        label += tokenizer.eos_token\n",
        "\n",
        "        new_examples['text'].append(input)\n",
        "        new_examples['labels'].append(label)\n",
        "    \n",
        "    return new_examples\n",
        "\n",
        "import datasets\n",
        "\n",
        "\n",
        "ds['cola'] = datasets.load_dataset(\"dlb/plue\",\"cola\",cache_dir=\"./cache\")\n",
        "\n",
        "identificador_de_dataset['cola']=criar_identificador()\n",
        "metric_functions['cola'] = cola_metric\n",
        "map_functions['cola'] = cola_map_fc\n",
        "ds['cola']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsWzoS7vJuKC"
      },
      "source": [
        "### MLNI\n",
        "\n",
        "Matched\n",
        "Train: 392702 exemplos, Validation: 9815 exemplos\n",
        "\n",
        "Mistmatched\n",
        "Validation: 9832 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNZ9LApsJvbw"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def mnli_matched_metric(predictions,labels):\n",
        "#     exact_match_metric = load(\"exact_match\")\n",
        "#     result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     return {'mnli_matched_acc': result['exact_match']}\n",
        "\n",
        "# def mnli_mismatched_metric(predictions,labels):\n",
        "#     exact_match_metric = load(\"exact_match\")\n",
        "#     result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     return {'mnli_mismatched_acc': result['exact_match']}\n",
        "\n",
        "# def mnli_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'smnli Premissa: {examples[\"premise\"][i]}'\n",
        "#         input+=f' Hipótese: {examples[\"hypothesis\"][i]}'\n",
        "#         if examples['label'][i] == 0:\n",
        "#             label = 'implicação lógica'\n",
        "#         elif examples['label'][i] == 1:\n",
        "#             label = 'neutro'\n",
        "#         elif examples['label'][i] == 2:\n",
        "#             label = 'contradição'\n",
        "#         elif examples['label'][i] == -1:\n",
        "#             # Este caso é o set de teste\n",
        "#             continue\n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' classe: ' + label\n",
        "\n",
        "#         label += tokenizer.eos_token\n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds_mlni = datasets.load_dataset(\"dlb/plue\",\"mnli\",cache_dir=\"./cache\")\n",
        "\n",
        "# ds['mnli_matched'] = datasets.DatasetDict({ 'train' : ds_mlni['train'], 'validation' : ds_mlni['validation_matched'],'test': ds_mlni['test_matched']})\n",
        "# ds['mnli_mismatched'] = datasets.DatasetDict({ 'validation' : ds_mlni['validation_mismatched'],'test': ds_mlni['test_mismatched']})\n",
        "# del ds_mlni\n",
        "\n",
        "# identificador_de_dataset['mnli_matched']=criar_identificador()\n",
        "# metric_functions['mnli_matched'] = mnli_matched_metric\n",
        "# map_functions['mnli_matched'] = mnli_map_fc\n",
        "\n",
        "# identificador_de_dataset['mnli_mismatched']=criar_identificador()\n",
        "# metric_functions['mnli_mismatched'] = mnli_mismatched_metric\n",
        "# map_functions['mnli_mismatched'] = mnli_map_fc\n",
        "# print('mnli_matched')\n",
        "# print(ds['mnli_matched'])\n",
        "# print('mnli_mismatched')\n",
        "# print(ds['mnli_mismatched'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X366MK0DCnJA"
      },
      "source": [
        "### MRPC - Microsoft Research Paraphrase Corpus\n",
        "\n",
        "Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators.\n",
        "\n",
        "https://paperswithcode.com/dataset/mrpc\n",
        "\n",
        "Train: 3668 exemplos, Validation: 408 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq8PZng2Csp-"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "def mrpc_metric(predictions,labels):\n",
        "    exact_match_metric = load(\"exact_match\")\n",
        "    result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "    return {'mrpc_acc': result['exact_match']}\n",
        "\n",
        "def mrpc_map_fc(examples):\n",
        "    new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "    first_key=list(examples.keys())[0]\n",
        "    for i in range(0,len(examples[first_key])):\n",
        "        input=f'mrpc As sentenças seguintes são similares ou diferentes? sentença 1: {examples[\"sentence1\"][i]}'\n",
        "        input+=f' sentença 2: {examples[\"sentence2\"][i]}'\n",
        "        if examples['label'][i] == 0:\n",
        "            label = 'diferentes'\n",
        "        elif examples['label'][i] == 1:\n",
        "            label = 'similares'\n",
        "        elif examples['label'][i] == -1:\n",
        "            # Este caso é o set de teste\n",
        "            continue\n",
        "\n",
        "        if model_type=='decoder':\n",
        "            if inserir_beginoftext_token:\n",
        "                input += target_bos_token\n",
        "            # else:\n",
        "            #     label = ' comparação das sentenças: ' + label\n",
        "        label += tokenizer.eos_token\n",
        "\n",
        "        new_examples['text'].append(input)\n",
        "        new_examples['labels'].append(label)\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "import datasets\n",
        "ds['mrpc'] = datasets.load_dataset(\"dlb/plue\",\"mrpc\",cache_dir=\"./cache\")\n",
        "\n",
        "identificador_de_dataset['mrpc']=criar_identificador()\n",
        "metric_functions['mrpc'] = mrpc_metric\n",
        "map_functions['mrpc'] = mrpc_map_fc\n",
        "ds['mrpc']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuhCCXZit1At"
      },
      "source": [
        "### QNLI_v2 - Question-answering NLI\n",
        "\n",
        "The QNLI (Question-answering NLI) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLEU benchmark.\n",
        "\n",
        "https://paperswithcode.com/dataset/qnli\n",
        "\n",
        "Train: 104743 exemplos, Validation: 5463 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRo-xuq4t2x6"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def qnli_v2_metric(predictions,labels):\n",
        "#     exact_match_metric = load(\"exact_match\")\n",
        "#     result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     return {'qnli_v2_acc': result['exact_match']}\n",
        "\n",
        "# def qnli_v2_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'qnli Pergunta: {examples[\"question\"][i]}'\n",
        "#         input+=f' Resposta: {examples[\"sentence\"][i]}'\n",
        "#         if examples['label'][i] == 0:\n",
        "#             label = 'Implicação'\n",
        "#         elif examples['label'][i] == 1:\n",
        "#             label = 'Não relacionadas'\n",
        "#         elif examples['label'][i] == -1:\n",
        "#             # Este caso é o set de teste\n",
        "#             continue\n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' classe: ' + label\n",
        "#         label += tokenizer.eos_token\n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds['qnli_v2'] = datasets.load_dataset(\"dlb/plue\",\"qnli_v2\",cache_dir=\"./cache\")\n",
        "\n",
        "# identificador_de_dataset['qnli_v2']=criar_identificador()\n",
        "# metric_functions['qnli_v2'] = qnli_v2_metric\n",
        "# map_functions['qnli_v2'] = qnli_v2_map_fc\n",
        "# ds['qnli_v2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFR3GUVFo0zA"
      },
      "source": [
        "### QQP_v2 - Quora Question Pairs\n",
        "Quora Question Pairs (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.\n",
        "\n",
        "Train: 363846 exemplos, Validation: 40430 exemplos\n",
        "\n",
        "https://paperswithcode.com/dataset/quora-question-pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CYwedmHpE0I"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def qqp_metric(predictions,labels):\n",
        "#     exact_match_metric = load(\"exact_match\")\n",
        "#     result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     return {'qqp_acc': result['exact_match']}\n",
        "\n",
        "# def qqp_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'qqp Pergunta 1: {examples[\"question1\"][i]}'\n",
        "#         input+=f' Pergunta 2: {examples[\"question2\"][i]}'\n",
        "#         if examples['label'][i] == 1:\n",
        "#             label = 'Não são perguntas duplicadas'\n",
        "#         elif examples['label'][i] == 0:\n",
        "#             label = 'São perguntas equivalentes'\n",
        "#         elif examples['label'][i] == -1:\n",
        "#             # Este caso é o set de teste\n",
        "#             continue\n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' classe: ' + label\n",
        "#         label += tokenizer.eos_token\n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds['qqp'] = datasets.load_dataset(\"dlb/plue\",\"qqp_v2\",cache_dir=\"./cache\")\n",
        "\n",
        "# identificador_de_dataset['qqp']=criar_identificador()\n",
        "# metric_functions['qqp'] = qqp_metric\n",
        "# map_functions['qqp'] = qqp_map_fc\n",
        "# ds['qqp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziY-tc30cxK2"
      },
      "source": [
        "### RTE\n",
        "\n",
        "Train: 2490 exemplos, Validation: 277 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGNRBUKQczCe"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "def rte_metric(predictions,labels):\n",
        "    exact_match_metric = load(\"exact_match\")\n",
        "    result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "    return {'rte_acc': result['exact_match']}\n",
        "\n",
        "def rte_map_fc(examples):\n",
        "    new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "    first_key=list(examples.keys())[0]\n",
        "    for i in range(0,len(examples[first_key])):\n",
        "        input=f'rte sentença 1: {examples[\"sentence1\"][i]}'\n",
        "        input+=f' sentença 2: {examples[\"sentence2\"][i]}'\n",
        "        if examples['label'][i] == 0:\n",
        "            label = 'É implicação lógica'\n",
        "        elif examples['label'][i] == 1:\n",
        "            label = 'Não é implicação lógica'\n",
        "        elif examples['label'][i] == -1:\n",
        "            # Este caso é o set de teste\n",
        "            continue\n",
        "\n",
        "        if model_type=='decoder':\n",
        "            if inserir_beginoftext_token:\n",
        "                input += target_bos_token\n",
        "            else:\n",
        "                label = ' conclusão: ' + label\n",
        "        label += tokenizer.eos_token\n",
        "\n",
        "        new_examples['text'].append(input)\n",
        "        new_examples['labels'].append(label)\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "import datasets\n",
        "ds['rte'] = datasets.load_dataset(\"dlb/plue\",\"rte\",cache_dir=\"./cache\")\n",
        "\n",
        "identificador_de_dataset['rte']=criar_identificador()\n",
        "metric_functions['rte'] = rte_metric\n",
        "map_functions['rte'] = rte_map_fc\n",
        "ds['rte']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_58i_pzEEgA"
      },
      "source": [
        "### SCITAIL\n",
        "\n",
        "The SciTail dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis.\n",
        "\n",
        "https://allenai.org/data/scitail\n",
        "\n",
        "Train: 23596 exemplos, Validation: 1304 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCrO4jYwEGEe"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def scitail_metric(predictions,labels):\n",
        "#     exact_match_metric = load(\"exact_match\")\n",
        "#     result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     return {'scitail_acc': result['exact_match']}\n",
        "\n",
        "# def scitail_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'scitail premissa: {examples[\"premise\"][i]}'\n",
        "#         input+=f' hipótese: {examples[\"hypothesis\"][i]}'\n",
        "#         if examples['label'][i] == 1:\n",
        "#             label = 'neutro'\n",
        "#         elif examples['label'][i] == 0:\n",
        "#             label = 'implicação lógica'\n",
        "#         else:\n",
        "#             continue        \n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' conclusão: ' + label\n",
        "#         label += tokenizer.eos_token  \n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds['scitail'] = datasets.load_dataset(\"dlb/plue\",\"scitail\",cache_dir=\"./cache\")\n",
        "\n",
        "# identificador_de_dataset['scitail']=criar_identificador()\n",
        "# metric_functions['scitail'] = scitail_metric\n",
        "# map_functions['scitail'] = scitail_map_fc\n",
        "# ds['scitail']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kkeIuaaICSx"
      },
      "source": [
        "### SNLI\n",
        "\n",
        "Train: 510711 exemplos, Validation: 9831 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxYsCpxLID8D"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def snli_metric(predictions,labels):\n",
        "#     exact_match_metric = load(\"exact_match\")\n",
        "#     result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     return {'snli_acc': result['exact_match']}\n",
        "\n",
        "# def snli_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'smnli Premissa: {examples[\"premise\"][i]}'\n",
        "#         input+=f' Hipótese: {examples[\"hypothesis\"][i]}'\n",
        "#         if examples['label'][i] == 0:\n",
        "#             label = 'implicação lógica'\n",
        "#         elif examples['label'][i] == 1:\n",
        "#             label = 'neutro'\n",
        "#         elif examples['label'][i] == 2:\n",
        "#             label = 'contradição'              \n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' conclusão: ' + label\n",
        "#         label += tokenizer.eos_token  \n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds['snli'] = datasets.load_dataset(\"dlb/plue\",\"snli\",cache_dir=\"./cache\")\n",
        "\n",
        "# identificador_de_dataset['snli']=criar_identificador()\n",
        "# metric_functions['snli'] = snli_metric\n",
        "# map_functions['snli'] = snli_map_fc\n",
        "# ds['snli']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMbOfVfAZW4d"
      },
      "source": [
        "### SST2\n",
        "\n",
        "Train: 67349 exemplos, Validation: 872 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pWuySQiZZt7"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def sst2_metric(predictions,labels):\n",
        "#     exact_match_metric = load(\"exact_match\")\n",
        "#     result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     return {'sst2_acc': result['exact_match']}\n",
        "\n",
        "# def sst2_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "#         input=f'sst2 sentença: {examples[\"sentence\"][i]}'\n",
        "\n",
        "#         if examples['label'][i] == 1:\n",
        "#             label = 'positivo'\n",
        "#         elif examples['label'][i] == 0:\n",
        "#             label = 'negativo'\n",
        "#         else:\n",
        "#             continue\n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' sentimento da frase: ' + label\n",
        "#         label += tokenizer.eos_token\n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds['sst2'] = datasets.load_dataset(\"dlb/plue\",\"sst2\",cache_dir=\"./cache\")\n",
        "\n",
        "# identificador_de_dataset['sst2']=criar_identificador()\n",
        "# metric_functions['sst2'] = sst2_metric\n",
        "# map_functions['sst2'] = sst2_map_fc\n",
        "# ds['sst2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO-0bS6ReBb3"
      },
      "source": [
        "### STSB\n",
        "\n",
        "Train: 5749 exemplos, Validation: 1379 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FPoPowceC7Z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from evaluate import load\n",
        "\n",
        "def stsb_metric(predictions,labels):\n",
        "    glue_metric = load('glue', 'stsb')\n",
        "\n",
        "    for i in range(0,len(predictions)):\n",
        "        # Busca o número na string gerada\n",
        "        pred_numbers = re.findall(r\"[-+]?(?:\\d*\\.*\\d+)\",predictions[i])\n",
        "        \n",
        "        # Caso não haja número, atribuir valor máximo e mínimo para penalizar o erros\n",
        "        if len(pred_numbers)==0:\n",
        "            predictions[i]=float(0)\n",
        "            labels[i]=float(5)\n",
        "        else:\n",
        "            try:\n",
        "                predictions[i] = float(pred_numbers[0])\n",
        "                labels[i] = float(labels[i])\n",
        "            except:\n",
        "                predictions[i]=float(0)\n",
        "                labels[i]=float(5)\n",
        "\n",
        "        if predictions[i]>5:\n",
        "            predictions[i]=5\n",
        "        elif predictions[i]<0:\n",
        "            predictions[i]=0\n",
        "\n",
        "    result = glue_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    mse_metric = load(\"mse\")\n",
        "    mse_results = mse_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {'stsb_pearson': result['pearson'], 'stsb_spearmanr': result['spearmanr'], 'stsb_mse': mse_results['mse']}\n",
        "\n",
        "def stsb_map_fc(examples):\n",
        "    new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "    first_key=list(examples.keys())[0]\n",
        "    for i in range(0,len(examples[first_key])):\n",
        "        input=f'stsb sentença 1: {examples[\"sentence1\"][i]}'\n",
        "        input+=f' sentença 2: {examples[\"sentence2\"][i]}'\n",
        "\n",
        "        if examples['label'][i] == -1:\n",
        "            continue\n",
        "        else:\n",
        "            label = ' ' + str(examples['label'][i])\n",
        "            # label = 'pontuação de similaridade: ' + str(examples['label'][i])\n",
        "\n",
        "        if model_type=='decoder':\n",
        "            if inserir_beginoftext_token:\n",
        "                input += target_bos_token\n",
        "            else:\n",
        "                label = ' pontuação de similaridade: ' + label\n",
        "        label += tokenizer.eos_token\n",
        "\n",
        "        new_examples['text'].append(input)\n",
        "        new_examples['labels'].append(label)\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "import datasets\n",
        "ds['stsb'] = datasets.load_dataset(\"dlb/plue\",\"stsb\",cache_dir=\"./cache\")\n",
        "\n",
        "identificador_de_dataset['stsb']=criar_identificador()\n",
        "metric_functions['stsb'] = stsb_metric\n",
        "map_functions['stsb'] = stsb_map_fc\n",
        "\n",
        "new_features = ds['stsb']['train'].features.copy()\n",
        "new_features['label'] = datasets.Value(dtype='string', id=None)\n",
        "ds['stsb'] = ds['stsb'].cast(new_features)\n",
        "\n",
        "ds['stsb']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5XNoYfOOxSz"
      },
      "source": [
        "### WNLI\n",
        "\n",
        "Train: 635 exemplos, Validation: 71 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaMK86-3OzDS"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "def wnli_metric(predictions,labels):\n",
        "    exact_match_metric = load(\"exact_match\")\n",
        "    result = exact_match_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "    return {'wnli_acc': result['exact_match']}\n",
        "\n",
        "def wnli_map_fc(examples):\n",
        "    new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "    first_key=list(examples.keys())[0]\n",
        "    for i in range(0,len(examples[first_key])):\n",
        "        input=f'wnli sentença 1: {examples[\"sentence1\"][i]}'\n",
        "        input+=f' sentença 2: {examples[\"sentence2\"][i]}'\n",
        "        if examples['label'][i] == 0:\n",
        "            label = 'Não é implicação lógica'\n",
        "        elif examples['label'][i] == 1:\n",
        "            label = 'Implicação lógica'\n",
        "        elif examples['label'][i] == -1:\n",
        "            # Este caso é o set de teste\n",
        "            continue\n",
        "\n",
        "        if model_type=='decoder':\n",
        "            if inserir_beginoftext_token:\n",
        "                input += target_bos_token\n",
        "            else:\n",
        "                label = ' classe: ' + label\n",
        "        label += tokenizer.eos_token\n",
        "\n",
        "        new_examples['text'].append(input)\n",
        "        new_examples['labels'].append(label)\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "import datasets\n",
        "ds['wnli'] = datasets.load_dataset(\"dlb/plue\",\"wnli\",cache_dir=\"./cache\")\n",
        "\n",
        "identificador_de_dataset['wnli']=criar_identificador()\n",
        "metric_functions['wnli'] = wnli_metric\n",
        "map_functions['wnli'] = wnli_map_fc\n",
        "ds['wnli']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhw46lfso5J3"
      },
      "source": [
        "## Sumarização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgXBnKPMlMoj"
      },
      "source": [
        "#### WIKI LINGUA\n",
        "\n",
        "sumarização de textos\n",
        "\n",
        "https://huggingface.co/datasets/wiki_lingua\n",
        "\n",
        "Train: 25328 exemplos, Validation: 2815 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt875qaflToz"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def wiki_lingua_metric(predictions,labels):\n",
        "#     rouge_metric = load('rouge')\n",
        "#     result = rouge_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     new_result = {}\n",
        "#     for key in result:\n",
        "#         new_result['wiki_lingua_' + key] = result[key]\n",
        "#     result = new_result\n",
        "\n",
        "#     return result\n",
        "\n",
        "# def wiki_lingua_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "\n",
        "#         for j in range(0,len(examples['article'][i]['document'])):\n",
        "#             titulo = examples['article'][i]['section_name'][j]\n",
        "#             documento = examples['article'][i]['document'][j]\n",
        "#             resumo = examples['article'][i]['summary'][j]\n",
        "\n",
        "#             input=f'Resuma o texto: {titulo}. {documento}'\n",
        "\n",
        "#             label = resumo\n",
        "\n",
        "#             if model_type=='decoder':\n",
        "#                 if inserir_beginoftext_token:\n",
        "#                     input += target_bos_token\n",
        "#                 else:\n",
        "#                     label = ' Resumo: ' + label\n",
        "#             label += tokenizer.eos_token\n",
        "\n",
        "#             new_examples['text'].append(input)\n",
        "#             new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds['wiki_lingua'] = datasets.load_dataset('wiki_lingua','portuguese')\n",
        "\n",
        "# identificador_de_dataset['wiki_lingua']=criar_identificador()\n",
        "# metric_functions['wiki_lingua'] = wiki_lingua_metric\n",
        "# map_functions['wiki_lingua'] = wiki_lingua_map_fc\n",
        "\n",
        "# ds['wiki_lingua'] = ds['wiki_lingua']['train'].train_test_split(test_size=0.1,seed=42)\n",
        "# ds['wiki_lingua'] = datasets.DatasetDict({ 'train': ds['wiki_lingua']['train'], 'validation':ds['wiki_lingua']['test']})\n",
        "# ds['wiki_lingua']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fAXv9P0FOZ2"
      },
      "source": [
        "### XLSUM\n",
        "\n",
        "https://huggingface.co/datasets/csebuetnlp/xlsum\n",
        "\n",
        "Train: 57402 exemplos, Validation: 7175 exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcTl0jqPFU-t"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "\n",
        "# def xlsum_metric(predictions,labels):\n",
        "#     rouge_metric = load('rouge')\n",
        "#     result = rouge_metric.compute(predictions=predictions,references=labels)\n",
        "\n",
        "#     new_result = {}\n",
        "#     for key in result:\n",
        "#         new_result['xlsum_' + key] = result[key]\n",
        "#     result = new_result\n",
        "\n",
        "#     return result\n",
        "\n",
        "# def xlsum_map_fc(examples):\n",
        "#     new_examples = { 'text':[], 'labels':[]}\n",
        "   \n",
        "#     first_key=list(examples.keys())[0]\n",
        "#     for i in range(0,len(examples[first_key])):\n",
        "\n",
        "#         input=f'Resuma o texto: {examples[\"title\"][i]}. {examples[\"text\"][i]}'\n",
        "\n",
        "#         label = examples['summary'][i]\n",
        "\n",
        "#         if model_type=='decoder':\n",
        "#             if inserir_beginoftext_token:\n",
        "#                 input += target_bos_token\n",
        "#             else:\n",
        "#                 label = ' Resumo: ' + label\n",
        "#         label += tokenizer.eos_token\n",
        "\n",
        "#         new_examples['text'].append(input)\n",
        "#         new_examples['labels'].append(label)\n",
        "\n",
        "#     return new_examples\n",
        "\n",
        "# import datasets\n",
        "# ds['xlsum'] = datasets.load_dataset('csebuetnlp/xlsum','portuguese')\n",
        "\n",
        "# identificador_de_dataset['xlsum']=criar_identificador()\n",
        "# metric_functions['xlsum'] = xlsum_metric\n",
        "# map_functions['xlsum'] = xlsum_map_fc\n",
        "# ds['xlsum']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO7KBycqaaqL"
      },
      "source": [
        "# Pré-processar o dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMduRnuEugjg"
      },
      "source": [
        "## Aplica a função map individual de cada dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjbjrw3bIwW2"
      },
      "outputs": [],
      "source": [
        "ds_processado = ds\n",
        "\n",
        "for key in ds.keys():\n",
        "\n",
        "    if type(map_functions[key])==dict:\n",
        "        # O dataset squad tem pecurialidade de ter diversas respostas possíveis\n",
        "        for split in map_functions[key]:\n",
        "            ds_processado[key][split] = ds[key][split].map(\n",
        "                map_functions[key][split],\n",
        "                batched=True,\n",
        "                batch_size=1_000,\n",
        "                remove_columns=ds[key][split].column_names,\n",
        "                num_proc=1\n",
        "            )\n",
        "    else:\n",
        "        ds_processado[key] = ds[key].map(\n",
        "            map_functions[key],\n",
        "            batched=True,\n",
        "            batch_size=1_000,\n",
        "            remove_columns=ds[key][list(ds[key].keys())[0]].column_names,\n",
        "            num_proc=1\n",
        "        )\n",
        "\n",
        "    # deleta os splits sem exemplos (e.g., o split 'test' do wnli)\n",
        "    delete_split=[]\n",
        "    for split in ds_processado[key]:\n",
        "        if len(ds_processado[key][split])==0:\n",
        "            delete_split.append(split)\n",
        "\n",
        "    for split in delete_split:\n",
        "        del ds_processado[key][split]\n",
        "\n",
        "ds_processado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XYfZMdiuasI"
      },
      "source": [
        "## Insere um prefixo prefix_input ao início dos inputs_ids, caso tenha sido definido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KqHl8_muQo6"
      },
      "outputs": [],
      "source": [
        "if 'prefix_input' in locals() and prefix_input!=None and len(prefix_input)>0:\n",
        "    def tokenize_dataset(examples):\n",
        "       # Acrescenta um prefixo nos input_ids para os modelos do tipo UL2, (prefixos como <|NLU|> ou <|NLG|>)\n",
        "        for i in range(0,len(examples['text'])):\n",
        "            examples['text'][i] = prefix_input + examples['text'][i]\n",
        "\n",
        "        return examples\n",
        "\n",
        "    for key in ds_processado.keys():\n",
        "        print(key)\n",
        "        ds_processado[key] = ds_processado[key].map(\n",
        "            tokenize_dataset,\n",
        "            batched=True,\n",
        "            batch_size=1_000,\n",
        "            num_proc=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krmYA8wbLXdZ"
      },
      "outputs": [],
      "source": [
        "for key in ds_processado:\n",
        "    print('dataset',key)\n",
        "    print(ds_processado[key][list(ds_processado[key].keys())[0]][0])\n",
        "    print(ds_processado[key][list(ds_processado[key].keys())[0]][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc8HsM9Ifhlz"
      },
      "source": [
        "## Tokeniza o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlcU29gCN4WF"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(examples):\n",
        "\n",
        "    examples['input_ids']=tokenizer(examples['text'],\n",
        "                      return_attention_mask=False,\n",
        "                      truncation=True,\n",
        "                      max_length=model.config.n_positions,\n",
        "                      )['input_ids']\n",
        "\n",
        "    examples['labels']=tokenizer(examples['labels'],\n",
        "                      return_attention_mask=False,\n",
        "                      truncation=True,\n",
        "                      max_length=model.config.n_positions,\n",
        "                      )['input_ids']\n",
        "    # Insere o eos_token_id caso não tenha sido inserido anteriormente\n",
        "    for i, label in enumerate(examples['labels']):\n",
        "        try:\n",
        "            if label[len(label)-1]!=tokenizer.eos_token_id:\n",
        "                examples['labels'][i] += [tokenizer.eos_token_id]            \n",
        "        except:\n",
        "            # Caso por erro não haja label\n",
        "            examples['labels'][i] = [tokenizer.eos_token_id]            \n",
        "            pass\n",
        "                      \n",
        "    return examples\n",
        "\n",
        "for key in ds_processado.keys():\n",
        "    print(key)\n",
        "    ds_tokenizado[key] = ds_processado[key].map(\n",
        "        tokenize_dataset,\n",
        "        batched=True,\n",
        "        batch_size=1_000,\n",
        "        num_proc=2\n",
        "    )\n",
        "    ds_tokenizado[key]=ds_tokenizado[key].remove_columns(\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-E5KVgcClW"
      },
      "source": [
        "## Insere um token inicial identificando qual é o dataset nos dados de validação para o computo das métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BJjL7yrRnJi"
      },
      "outputs": [],
      "source": [
        "def inserir_identificador(examples):\n",
        "    if model_type=='encoder-decoder':  \n",
        "        for i in range(0,len(examples['labels'])):\n",
        "            examples['labels'][i]=[identificador_de_dataset[dataset_name]] + examples['labels'][i]\n",
        "\n",
        "    # no decoder, inserimos no começo tanto o identificador do dataset quanto o tamanho do input inicial para a remoção\n",
        "    elif model_type=='decoder':  \n",
        "        for i in range(0,len(examples['labels'])):\n",
        "            examples['labels'][i] = [identificador_de_dataset[dataset_name]] + [len(examples['input_ids'][i])]  + examples['labels'][i]\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kkuccnZdZ_D"
      },
      "outputs": [],
      "source": [
        "for key in ds_tokenizado.keys():\n",
        "    dataset_name=key\n",
        "    if 'test' in ds_tokenizado[key]:\n",
        "        ds_tokenizado[key]['test'] = ds_tokenizado[key]['test'].map(\n",
        "            inserir_identificador,\n",
        "            batched=True,\n",
        "            batch_size=1_000,\n",
        "            num_proc=1\n",
        "        )\n",
        "    ds_tokenizado[key]['validation']= ds_tokenizado[key]['validation'].map(\n",
        "        inserir_identificador,\n",
        "        batched=True,\n",
        "        batch_size=1_000,\n",
        "        num_proc=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ofuLVqkVzP"
      },
      "source": [
        "## Concatena os diversos datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7_Rt0LIOPvM"
      },
      "outputs": [],
      "source": [
        "concat={ 'train':[],'validation':[], 'test':[]}\n",
        "\n",
        "for key in ds_tokenizado:\n",
        "    for split in ds_tokenizado[key].keys():\n",
        "        concat[split].append(ds_tokenizado[key][split])\n",
        "\n",
        "# Concatena os datasets e randomiza a ordem\n",
        "ds_final = datasets.DatasetDict({\n",
        "    'train': datasets.concatenate_datasets(concat['train']),\n",
        "    'validation': datasets.concatenate_datasets(concat['validation']),\n",
        "})\n",
        "\n",
        "ds_final.shuffle()\n",
        "ds_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8aCJrqlYhCJ"
      },
      "source": [
        "# Cria a métrica para avaliar os datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wKLy-1j_u7g"
      },
      "source": [
        "## Métrica do encoder-decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfGbxKD_YhKp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Identifica qual o dataset\n",
        "def retorna_chave_do_dicionario(dictionary, search_value):\n",
        "    for key, value in dictionary.items():\n",
        "        if value == search_value:\n",
        "            return key\n",
        "\n",
        "# Cálculo das métricas por dataset\n",
        "def compute_metrics_encoder_decoder(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    result = {}\n",
        "    \n",
        "    dataset_name=[]\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    predictions=list(predictions)\n",
        "    labels=list(labels)\n",
        "\n",
        "    # identifica qual é o dataset, e remove o identificador\n",
        "    for i in range(0,len(labels)):\n",
        "        identificador=labels[i][0]        \n",
        "        dataset_name.append(retorna_chave_do_dicionario(identificador_de_dataset,identificador))\n",
        "        labels[i]=labels[i][1:len(labels[i])]             \n",
        "    \n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # print('Decoded labels')\n",
        "    # print(decoded_labels)\n",
        "    # print('Decoded predictions')\n",
        "    # print(decoded_preds)\n",
        "\n",
        "    # inicaliza o dicionário\n",
        "    decoded_por_dataset = {}\n",
        "\n",
        "    for key in identificador_de_dataset:\n",
        "        decoded_por_dataset[key]={}\n",
        "        decoded_por_dataset[key]['decoded_labels']=[]\n",
        "        decoded_por_dataset[key]['decoded_preds']=[]\n",
        "    \n",
        "    for i in range(0,len(decoded_labels)):\n",
        "        key = dataset_name[i]\n",
        "        decoded_por_dataset[key]['decoded_labels'].append(decoded_labels[i])\n",
        "        decoded_por_dataset[key]['decoded_preds'].append(decoded_preds[i])\n",
        "    \n",
        "    result = {}\n",
        "    for key in decoded_por_dataset:\n",
        "        # print('dataset',key)\n",
        "        # print('examples',len(decoded_por_dataset[key]['decoded_labels']))\n",
        "        res = metric_functions[key](decoded_por_dataset[key]['decoded_preds'],decoded_por_dataset[key]['decoded_labels'])\n",
        "        \n",
        "        result.update(res)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NY0qtSQ_xzt"
      },
      "source": [
        "## Métrica do decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00eXd3u8_-Em"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def retorna_chave_do_dicionario(dictionary, search_value):\n",
        "    for key, value in dictionary.items():\n",
        "        if value == search_value:\n",
        "            return key\n",
        "\n",
        "def compute_metrics_decoder(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    result = {}\n",
        "    \n",
        "    dataset_name=[]    \n",
        "\n",
        "    predictions=list(predictions)\n",
        "    labels=list(labels)\n",
        "    \n",
        "    for i in range(0,len(labels)):\n",
        "        labels[i] = list(filter(lambda x: x!= -100, labels[i]))\n",
        "        predictions[i] = list(filter(lambda x: x!= -100, predictions[i]))\n",
        "\n",
        "        # remove os pad_tokens\n",
        "        labels[i] = list(filter(lambda x: x!= tokenizer.pad_token_id, labels[i]))\n",
        "        predictions[i] = list(filter(lambda x: x!= tokenizer.pad_token_id, predictions[i]))\n",
        "\n",
        "        # remove os eos_tokens\n",
        "        labels[i] = list(filter(lambda x: x!= tokenizer.eos_token_id, labels[i]))\n",
        "        predictions[i] = list(filter(lambda x: x!= tokenizer.eos_token_id, predictions[i]))\n",
        "\n",
        "    # identifica qual é o dataset, e remove o identificador\n",
        "    for i in range(0,len(labels)):\n",
        "        identificador=labels[i][0]\n",
        "        dataset_name.append(retorna_chave_do_dicionario(identificador_de_dataset,identificador))\n",
        "        labels[i]=labels[i][1:len(labels[i])]  \n",
        "\n",
        "    # remove o começo da string referente ao input\n",
        "    for i in range(0,len(labels)):\n",
        "        length_input=labels[i][0]\n",
        "\n",
        "        labels[i]=labels[i][1:len(labels[i])]\n",
        "        predictions[i]=predictions[i][length_input:len(predictions[i])]\n",
        "\n",
        "    \n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # print('Decoded labels')\n",
        "    # print(decoded_labels)\n",
        "    # print('Decoded predictions')\n",
        "    # print(decoded_preds)\n",
        "\n",
        "\n",
        "    # inicaliza o dicionário\n",
        "    decoded_por_dataset = {}\n",
        "\n",
        "    for key in identificador_de_dataset:\n",
        "        decoded_por_dataset[key]={}\n",
        "        decoded_por_dataset[key]['decoded_labels']=[]\n",
        "        decoded_por_dataset[key]['decoded_preds']=[]\n",
        "    \n",
        "    for i in range(0,len(decoded_labels)):\n",
        "        key = dataset_name[i]\n",
        "        decoded_por_dataset[key]['decoded_labels'].append(decoded_labels[i])\n",
        "        decoded_por_dataset[key]['decoded_preds'].append(decoded_preds[i])\n",
        "\n",
        "    result = {}\n",
        "    for key in decoded_por_dataset:\n",
        "        # print('dataset',key)\n",
        "        # print('examples',len(decoded_por_dataset[key]['decoded_labels']))\n",
        "        res = metric_functions[key](decoded_por_dataset[key]['decoded_preds'],decoded_por_dataset[key]['decoded_labels'])\n",
        "        \n",
        "        result.update(res)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmOgli9IKGE7"
      },
      "source": [
        "# DataCollator para Decoders (Causal Language Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enquanto os modelos encoder-decoder possuem o input e os outputs separados, nos modelos decoder, só há um vetor de texto.\n",
        "\n",
        "No finetunning para classificação dos modelos decoders, colocaremos para o modelo apenas prever o texto do label. Assim, a parte do input será atribuída um label de valor -100, assim o modelo saberá que não deve ser calculado *loss* para esses tokens."
      ],
      "metadata": {
        "id": "UBK4uoS8o_fC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqCUqkohKLnH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import warnings\n",
        "from collections.abc import Mapping\n",
        "from dataclasses import dataclass\n",
        "from random import randint\n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from transformers.models.bert import BertTokenizer, BertTokenizerFast\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from transformers.utils import PaddingStrategy\n",
        "\n",
        "import torch\n",
        "import transformers.data.data_collator\n",
        "from transformers.data.data_collator import _torch_collate_batch\n",
        "\n",
        "\n",
        "class DataCollatorMixin:\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "        if return_tensors is None:\n",
        "            return_tensors = self.return_tensors\n",
        "        if return_tensors == \"tf\":\n",
        "            return self.tf_call(features)\n",
        "        elif return_tensors == \"pt\":\n",
        "            return self.torch_call(features)\n",
        "        elif return_tensors == \"np\":\n",
        "            return self.numpy_call(features)\n",
        "        else:\n",
        "            raise ValueError(f\"Framework '{return_tensors}' not recognized!\")\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorWithPaddingModified:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
        "            The tokenizer used for encoding the data.\n",
        "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence is provided).\n",
        "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
        "              acceptable input length for the model if that argument is not provided.\n",
        "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
        "        max_length (`int`, *optional*):\n",
        "            Maximum length of the returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (`int`, *optional*):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "        return_tensors (`str`):\n",
        "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:        \n",
        "        inputs=[]\n",
        "        labels=[]\n",
        "        attention_mask=[]\n",
        "        \n",
        "        # Utiliza o primeiro token para identificar se é o dataset de validação\n",
        "        is_validation_dataset = (features[0]['labels'][0] > len(tokenizer))\n",
        "        if is_validation_dataset:\n",
        "            i = 0\n",
        "            for feat in features:\n",
        "                labels.append(feat['labels'])\n",
        "                inputs.append(feat['input_ids'])   \n",
        "\n",
        "        else: # training datset       \n",
        "            for feat in features:\n",
        "                labels.append([-100] * len(feat['input_ids']) + feat['labels'])\n",
        "                inputs.append(feat['input_ids'] + feat['labels'])\n",
        "\n",
        "        # artifício para dar pad nos inputs e labels ao mesmo tempo\n",
        "        inputs = {'input_ids' : inputs + labels}\n",
        "        \n",
        "        previous_level = transformers.logging.get_verbosity()\n",
        "        transformers.logging.set_verbosity_error()  \n",
        "\n",
        "        batch = self.tokenizer.pad(\n",
        "            inputs,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=self.return_tensors,\n",
        "        )\n",
        "        \n",
        "        transformers.logging.set_verbosity(previous_level) ####\n",
        "\n",
        "        half_idx = len(labels)\n",
        "\n",
        "        batch['labels'] = batch['input_ids'][half_idx:len(batch['input_ids'])]\n",
        "        batch['input_ids'] = batch['input_ids'][0:half_idx]\n",
        "        batch['attention_mask'] = batch['attention_mask'][0:half_idx]\n",
        "\n",
        "        \n",
        "        batch['labels'][batch['labels'] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        if \"label\" in batch:\n",
        "            batch[\"labels\"] = batch[\"label\"]\n",
        "            del batch[\"label\"]\n",
        "        if \"label_ids\" in batch:\n",
        "            batch[\"labels\"] = batch[\"label_ids\"]\n",
        "            del batch[\"label_ids\"]\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtSXxiLnXcNB"
      },
      "source": [
        "# DataCollator para Encoder-Decoder\n",
        "\n",
        "A única modificação feita do código original é silenciar o tokenizador durante o pad\n",
        "https://github.com/huggingface/transformers/blob/v4.28.1/src/transformers/data/data_collator.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42XpuBUfXcgw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import warnings\n",
        "from collections.abc import Mapping\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
        "\n",
        "from transformers.models.bert import BertTokenizer, BertTokenizerFast\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from transformers.utils import PaddingStrategy\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSeq2SeqModified:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
        "    Args:\n",
        "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
        "            The tokenizer used for encoding the data.\n",
        "        model ([`PreTrainedModel`]):\n",
        "            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to\n",
        "            prepare the *decoder_input_ids*\n",
        "            This is useful when using *label_smoothing* to avoid calculating loss twice.\n",
        "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence is provided).\n",
        "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
        "              acceptable input length for the model if that argument is not provided.\n",
        "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
        "        max_length (`int`, *optional*):\n",
        "            Maximum length of the returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (`int`, *optional*):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
        "            The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).\n",
        "        return_tensors (`str`):\n",
        "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    model: Optional[Any] = None\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    label_pad_token_id: int = -100\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "        if return_tensors is None:\n",
        "            return_tensors = self.return_tensors\n",
        "        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
        "\n",
        "        previous_level = transformers.logging.get_verbosity()\n",
        "        transformers.logging.set_verbosity_error()     \n",
        "\n",
        "        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
        "        # same length to return tensors.\n",
        "        if labels is not None:\n",
        "            max_label_length = max(len(l) for l in labels)\n",
        "            if self.pad_to_multiple_of is not None:\n",
        "                max_label_length = (\n",
        "                    (max_label_length + self.pad_to_multiple_of - 1)\n",
        "                    // self.pad_to_multiple_of\n",
        "                    * self.pad_to_multiple_of\n",
        "                )\n",
        "\n",
        "            padding_side = self.tokenizer.padding_side\n",
        "            for feature in features:\n",
        "                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n",
        "                if isinstance(feature[\"labels\"], list):\n",
        "                    feature[\"labels\"] = (\n",
        "                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
        "                    )\n",
        "                elif padding_side == \"right\":\n",
        "                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n",
        "                else:\n",
        "                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n",
        "\n",
        "        features = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=return_tensors,\n",
        "        )\n",
        "        \n",
        "\n",
        "        # prepare decoder_input_ids\n",
        "        if (\n",
        "            labels is not None\n",
        "            and self.model is not None\n",
        "            and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n",
        "        ):\n",
        "            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n",
        "            features[\"decoder_input_ids\"] = decoder_input_ids\n",
        "\n",
        "        transformers.logging.set_verbosity(previous_level) ####\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPYv_RDwUN91"
      },
      "source": [
        "# Treina o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdyvQDUPiBFw"
      },
      "source": [
        "## Ajusta a classe Trainer do hugginface\n",
        "\n",
        "Foi alterada na classe Trainer a configuração de geração de textos de validação e silenciado os avisos na geração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waZy7Ee_0jvY"
      },
      "outputs": [],
      "source": [
        "# https://github.com/huggingface/transformers/blob/v4.26.1/src/transformers/trainer_seq2seq.py\n",
        "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers.deepspeed import is_deepspeed_zero3_enabled\n",
        "from transformers.trainer import Trainer\n",
        "from transformers.trainer_utils import PredictionOutput\n",
        "from transformers.utils import logging\n",
        "import transformers\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "class Seq2SeqTrainerModified(Trainer):\n",
        "    def evaluate(\n",
        "        self,\n",
        "        eval_dataset: Optional[Dataset] = None,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"eval\",\n",
        "        **gen_kwargs\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Run evaluation and returns metrics.\n",
        "        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
        "        (pass it to the init `compute_metrics` argument).\n",
        "        You can also subclass and override this method to inject custom behavior.\n",
        "        Args:\n",
        "            eval_dataset (`Dataset`, *optional*):\n",
        "                Pass a dataset if you wish to override `self.eval_dataset`. If it is an [`~datasets.Dataset`], columns\n",
        "                not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n",
        "                method.\n",
        "            ignore_keys (`List[str]`, *optional*):\n",
        "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
        "                gathering predictions.\n",
        "            metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
        "                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
        "                \"eval_bleu\" if the prefix is `\"eval\"` (default)\n",
        "            max_length (`int`, *optional*):\n",
        "                The maximum target length to use when predicting with the generate method.\n",
        "            num_beams (`int`, *optional*):\n",
        "                Number of beams for beam search that will be used when predicting with the generate method. 1 means no\n",
        "                beam search.\n",
        "            gen_kwargs:\n",
        "                Additional `generate` specific kwargs.\n",
        "        Returns:\n",
        "            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
        "            dictionary also contains the epoch number which comes from the training state.\n",
        "        \"\"\"\n",
        "\n",
        "        gen_kwargs = gen_kwargs.copy()\n",
        "        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
        "            gen_kwargs[\"max_length\"] = self.args.generation_max_length\n",
        "        gen_kwargs[\"num_beams\"] = (\n",
        "            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.args.generation_num_beams\n",
        "        )\n",
        "        self._gen_kwargs = gen_kwargs\n",
        "\n",
        "        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        test_dataset: Dataset,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "        metric_key_prefix: str = \"test\",\n",
        "        **gen_kwargs\n",
        "    ) -> PredictionOutput:\n",
        "        \"\"\"\n",
        "        Run prediction and returns predictions and potential metrics.\n",
        "        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n",
        "        will also return metrics, like in `evaluate()`.\n",
        "        Args:\n",
        "            test_dataset (`Dataset`):\n",
        "                Dataset to run the predictions on. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
        "                `model.forward()` method are automatically removed. Has to implement the method `__len__`\n",
        "            ignore_keys (`List[str]`, *optional*):\n",
        "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
        "                gathering predictions.\n",
        "            metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
        "                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
        "                \"eval_bleu\" if the prefix is `\"eval\"` (default)\n",
        "            max_length (`int`, *optional*):\n",
        "                The maximum target length to use when predicting with the generate method.\n",
        "            num_beams (`int`, *optional*):\n",
        "                Number of beams for beam search that will be used when predicting with the generate method. 1 means no\n",
        "                beam search.\n",
        "            gen_kwargs:\n",
        "                Additional `generate` specific kwargs.\n",
        "        <Tip>\n",
        "        If your predictions or labels have different sequence lengths (for instance because you're doing dynamic\n",
        "        padding in a token classification task) the predictions will be padded (on the right) to allow for\n",
        "        concatenation into one array. The padding index is -100.\n",
        "        </Tip>\n",
        "        Returns: *NamedTuple* A namedtuple with the following keys:\n",
        "            - predictions (`np.ndarray`): The predictions on `test_dataset`.\n",
        "            - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n",
        "            - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n",
        "              labels).\n",
        "        \"\"\"\n",
        "\n",
        "        gen_kwargs = gen_kwargs.copy()\n",
        "        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
        "            gen_kwargs[\"max_length\"] = self.args.generation_max_length\n",
        "        gen_kwargs[\"num_beams\"] = (\n",
        "            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.args.generation_num_beams\n",
        "        )\n",
        "        self._gen_kwargs = gen_kwargs\n",
        "\n",
        "        return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
        "\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Perform an evaluation step on `model` using `inputs`.\n",
        "        Subclass and override to inject custom behavior.\n",
        "        Args:\n",
        "            model (`nn.Module`):\n",
        "                The model to evaluate.\n",
        "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
        "                The inputs and targets of the model.\n",
        "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
        "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
        "            prediction_loss_only (`bool`):\n",
        "                Whether or not to return the loss only.\n",
        "        Return:\n",
        "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n",
        "            labels (each being optional).\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.args.predict_with_generate or prediction_loss_only:\n",
        "            return super().prediction_step(\n",
        "                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n",
        "            )\n",
        "\n",
        "        has_labels = \"labels\" in inputs\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # XXX: adapt synced_gpus for fairscale as well\n",
        "        gen_kwargs = self._gen_kwargs.copy()\n",
        "        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
        "            gen_kwargs[\"max_length\"] = self.model.config.max_length\n",
        "        gen_kwargs[\"num_beams\"] = (\n",
        "            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.model.config.num_beams\n",
        "        )\n",
        "        default_synced_gpus = True if is_deepspeed_zero3_enabled() else False\n",
        "        gen_kwargs[\"synced_gpus\"] = (\n",
        "            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n",
        "        )\n",
        "\n",
        "        if \"attention_mask\" in inputs:\n",
        "            gen_kwargs[\"attention_mask\"] = inputs.get(\"attention_mask\", None)\n",
        "        if \"global_attention_mask\" in inputs:\n",
        "            gen_kwargs[\"global_attention_mask\"] = inputs.get(\"global_attention_mask\", None)\n",
        "\n",
        "        # prepare generation inputs\n",
        "        # some encoder-decoder models can have varying encoder's and thus\n",
        "        # varying model input names\n",
        "        if hasattr(self.model, \"encoder\") and self.model.encoder.main_input_name != self.model.main_input_name:\n",
        "            generation_inputs = inputs[self.model.encoder.main_input_name]\n",
        "        else:\n",
        "            generation_inputs = inputs[self.model.main_input_name]\n",
        "\n",
        "        ##### Alteração\n",
        "        gen_kwargs[\"max_new_tokens\"] = MAX_TOKEN_GENERATION_LENGTH\n",
        "        del gen_kwargs[\"max_length\"]\n",
        "        gen_kwargs[\"eos_token_id\"]=self.tokenizer.eos_token_id\n",
        "        previous_level = transformers.logging.get_verbosity()\n",
        "        transformers.logging.set_verbosity_error()        \n",
        "        #####        \n",
        "        generated_tokens = self.model.generate(\n",
        "            generation_inputs,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "        transformers.logging.set_verbosity(previous_level) ####\n",
        "        \n",
        "\n",
        "        # in case the batch is shorter than max length, the output should be padded\n",
        "        if gen_kwargs.get(\"max_length\") is not None and generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
        "        elif gen_kwargs.get(\"max_new_tokens\") is not None and generated_tokens.shape[-1] < (\n",
        "            gen_kwargs[\"max_new_tokens\"] + 1\n",
        "        ):\n",
        "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_new_tokens\"] + 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if has_labels:\n",
        "                with self.compute_loss_context_manager():\n",
        "                    outputs = model(**inputs)\n",
        "                if self.label_smoother is not None:\n",
        "                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
        "                else:\n",
        "                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
        "            else:\n",
        "                loss = None\n",
        "\n",
        "        if self.args.prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "\n",
        "        if has_labels:\n",
        "            labels = inputs[\"labels\"]\n",
        "            if gen_kwargs.get(\"max_length\") is not None and labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "                labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
        "            elif gen_kwargs.get(\"max_new_tokens\") is not None and labels.shape[-1] < (\n",
        "                gen_kwargs[\"max_new_tokens\"] + 1\n",
        "            ):\n",
        "                labels = self._pad_tensors_to_max_len(labels, (gen_kwargs[\"max_new_tokens\"] + 1))\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        transformers.logging.set_verbosity(previous_level) ####\n",
        "        return (loss, generated_tokens, labels)\n",
        "\n",
        "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
        "        if self.tokenizer is not None and hasattr(self.tokenizer, \"pad_token_id\"):\n",
        "            # If PAD token is not defined at least EOS token has to be defined\n",
        "            pad_token_id = (\n",
        "                self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
        "            )\n",
        "        else:\n",
        "            if self.model.config.pad_token_id is not None:\n",
        "                pad_token_id = self.model.config.pad_token_id\n",
        "            else:\n",
        "                raise ValueError(\"Pad_token_id must be set in the configuration of the model, in order to pad tensors\")\n",
        "\n",
        "        padded_tensor = pad_token_id * torch.ones(\n",
        "            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n",
        "        )\n",
        "        padded_tensor[:, : tensor.shape[-1]] = tensor\n",
        "        return padded_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jgrlcn-C_Gs"
      },
      "source": [
        "## Treina o model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AcBhmIJBory"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "if model_type=='decoder':\n",
        "    data_collator = DataCollatorWithPaddingModified(tokenizer,max_length=model.config.n_positions,pad_to_multiple_of=8,return_tensors='pt')\n",
        "    compute_metrics=compute_metrics_decoder    \n",
        "    learning_rate=1e-4\n",
        "elif model_type=='encoder-decoder':\n",
        "    data_collator = DataCollatorForSeq2SeqModified(tokenizer,model=model,max_length=model.config.n_positions,pad_to_multiple_of=8,return_tensors='pt')\n",
        "    compute_metrics=compute_metrics_encoder_decoder\n",
        "    learning_rate=1e-4\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    # save_strategy=\"epoch\",        \n",
        "    # save_total_limit=10,\n",
        "    # load_best_model_at_end=True,\n",
        "\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    eval_steps=1,\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    # eval_steps=int(len(ds_final['train'])//5),\n",
        "    logging_strategy=\"epoch\",\n",
        "    logging_steps=1,\n",
        "    predict_with_generate=True,\n",
        "    \n",
        "\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,        \n",
        "\n",
        "    num_train_epochs=epochs,\n",
        "\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=0.1,\n",
        "    \n",
        "    fp16=True,\n",
        "    fp16_full_eval=False,\n",
        "    dataloader_num_workers=1,   \n",
        ")\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainerModified(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=ds_final[\"train\"],\n",
        "    eval_dataset=ds_final[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOSO48cz5XJG"
      },
      "source": [
        "# Gera texto pelo modelo finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKf0PGC-BFWm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "texts=[]\n",
        "\n",
        "for key in ds_processado.keys():\n",
        "    if 'train' in ds_processado[key]: \n",
        "        texts.append(ds_processado[key]['train'][0]['text'])\n",
        "        texts.append(ds_processado[key]['train'][1]['text'])\n",
        "\n",
        "    texts.append(ds_processado[key]['validation'][0]['text'])\n",
        "    texts.append(ds_processado[key]['validation'][1]['text'])\n",
        "\n",
        "\n",
        "model.to('cpu')\n",
        "pred=[]\n",
        "for text in texts:    \n",
        "    pred.append(tokenizer.batch_decode(model.generate(tokenizer.encode(text,return_tensors='pt'),max_new_tokens=20,eos_token_id=tokenizer.eos_token_id)))\n",
        "\n",
        "for i in range(0,len(texts)):\n",
        "    print('input:',texts[i])\n",
        "    print('generated:',pred[i])\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrOh9N02Su-_"
      },
      "source": [
        "# Desconectar do COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbosINFq2VhP"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}